{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc38325",
   "metadata": {},
   "source": [
    "# Understanding the concept :-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae597dd",
   "metadata": {},
   "source": [
    "# 1. torch.AvgPool3d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5dda011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 6, 254, 254])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "avg_pool = nn.AvgPool3d(kernel_size=3,\n",
    "                        stride=1)\n",
    "\n",
    "x = torch.randn(2, 3, 8, 256, 256)\n",
    "\n",
    "\n",
    "# frame =  [D_in + 2 * padding[0] - kernel_size[0] / stride[0]] + 1\n",
    "# height =  [h_in + 2 * padding[1] - kernel_size[1] / stride[1]] + 1\n",
    "# width =  [h_in + 2 * padding[2] - kernel_size[2] / stride[2]] + 1\n",
    "\n",
    "avg_pool = avg_pool(x)\n",
    "avg_pool.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9529b39a",
   "metadata": {},
   "source": [
    "**Depth Dimension CALCULATION**: \n",
    "```D_in = 8 \n",
    "padding = [0, 0, 0]\n",
    "kernel_size = [3, 3, 3]\n",
    "stride = [1, 1, 1]\n",
    "\n",
    "frame = D_in + 2 * padding[0] - kernel_size[0] / stride[0]\n",
    "frame = frame + 1 \n",
    "frame```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c249a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 6, 254, 254])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "conv = torch.nn.Conv3d(in_channels=3,\n",
    "                       out_channels=3,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=0)\n",
    "\n",
    "x = torch.randn(2, 3, 8, 256, 256)\n",
    "\n",
    "output = conv(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23364451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the input_channels: >>>>>> 128\n",
      "what is the input_channels: 128 and output_channels: 128\n",
      "what is the input_channels: 128 and output_channels: 256\n",
      "what is the input_channels: 256 and output_channels: 512\n",
      "what is the input_channels: 512 and output_channels: 512\n"
     ]
    }
   ],
   "source": [
    "block_out_channels = (128, 256, 512, 512,)\n",
    "down_block_type = (\"DownBlock\", \"DownBlock\", \"DownBlock\", \"DownBlock\",)\n",
    "\n",
    "output_channels = block_out_channels[0]\n",
    "print(f\"what is the input_channels: >>>>>> {output_channels}\")\n",
    "        # self.down_blocks = nn.ModuleList([])\n",
    "for i, down_block_type in enumerate(down_block_type):\n",
    "        input_channels = output_channels\n",
    "        output_channels = block_out_channels[i]\n",
    "        print(f\"what is the input_channels: {input_channels} and output_channels: {output_channels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150efa25",
   "metadata": {},
   "source": [
    "## 2. super().forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12790a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 256, 256])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "class BaseCNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=3,\n",
    "                              out_channels=3,\n",
    "                              kernel_size=1,\n",
    "                              )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x \n",
    "    \n",
    "\n",
    "\n",
    "base_cnn = BaseCNN()\n",
    "x = torch.randn(2, 3, 256, 256)\n",
    "output = base_cnn(x)\n",
    "output.shape    # (2, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b492c23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 256, 256])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomCNN(BaseCNN):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # the `BaseCNN` class to inherit the method \n",
    "        return super().forward(x)\n",
    "    \n",
    "\n",
    "\n",
    "base_cnn = BaseCNN()\n",
    "x = torch.randn(2, 3, 256, 256)\n",
    "output = base_cnn(x)\n",
    "output.shape    # (2, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e28ebc",
   "metadata": {},
   "source": [
    "# 3. enumerate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54de0e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index : 0 and the name: manish\n",
      "index : 1 and the name: anshu\n",
      "index : 2 and the name: ram\n"
     ]
    }
   ],
   "source": [
    "names = [\"manish\", \"anshu\", \"ram\"]\n",
    "\n",
    "for i, name in enumerate(names):\n",
    "    print(f\"index : {i} and the name: {name}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8efafe",
   "metadata": {},
   "source": [
    "## 4. nn.Module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e50fb6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Randomclass' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m\n\u001b[1;32m     25\u001b[0m random_class \u001b[38;5;241m=\u001b[39m Randomclass(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     26\u001b[0m                            out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m) \u001b[38;5;66;03m# image size \u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m output\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Randomclass' object is not callable"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class Randomclass:\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels,\n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=2,\n",
    "                              )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv(x)\n",
    "        return x \n",
    "    \n",
    "\n",
    "\n",
    "random_class = Randomclass(in_channels=3,\n",
    "                           out_channels=3)\n",
    "\n",
    "x = torch.randn(2, 3, 256, 256) # image size \n",
    "\n",
    "output = random_class(x)\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cca87e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 255, 255])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class Randomclass(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels,\n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=2,\n",
    "                              )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv(x)\n",
    "        return x \n",
    "    \n",
    "\n",
    "\n",
    "random_class = Randomclass(in_channels=3,\n",
    "                           out_channels=3)\n",
    "\n",
    "x = torch.randn(2, 3, 256, 256) # image size \n",
    "\n",
    "output = random_class(x)\n",
    "output.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc604fc8",
   "metadata": {},
   "source": [
    "## 5. stride()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad5c68",
   "metadata": {},
   "source": [
    "i follow this page: https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv3d.html\n",
    "\n",
    "```\n",
    "        import math \n",
    "\n",
    "        depth = 8\n",
    "        stride = 2 \n",
    "        padding = 0 # default\n",
    "        kernel_size = 3 # default\n",
    "        dilation = 1 # default \n",
    "        batch_size = 2\n",
    "\n",
    "        calculate_stride = ((depth + batch_size*padding - dilation * (kernel_size - 1) - 1) + 1) / stride\n",
    "        calculate_stride\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6862422f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 3, 254, 254])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "stride = (2, 1, 1)\n",
    "\n",
    "\n",
    "conv = nn.Conv3d(in_channels=128,\n",
    "                 out_channels=128,\n",
    "                 kernel_size=3,\n",
    "                 stride=stride)\n",
    "\n",
    "x = torch.randn(2, 128, 8, 256, 256)\n",
    "\n",
    "output = conv(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22b436eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 8, 128, 128])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "stride = (1, 2, 2)\n",
    "\n",
    "\n",
    "conv = nn.Conv3d(in_channels=128,\n",
    "                 out_channels=128,\n",
    "                 kernel_size=3,\n",
    "                 stride=stride,\n",
    "                 padding=1) # this padding are applied in forward function \n",
    "\n",
    "x = torch.randn(2, 128, 8, 256, 256)\n",
    "\n",
    "output = conv(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8e8e70",
   "metadata": {},
   "source": [
    "## 6. loop zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaee7fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latter: a and number: 1\n",
      "latter: b and number: 2\n",
      "latter: c and number: 3\n"
     ]
    }
   ],
   "source": [
    "list1 = ['a', 'b', 'c']\n",
    "list2 = [1, 2, 3]\n",
    "\n",
    "for latter, number in zip(list1, list2):\n",
    "    print(f\"latter: {latter} and number: {number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed0e367",
   "metadata": {},
   "source": [
    "## 7. torch.pad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41b91813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 256, 258])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch.nn.functional import pad as Padding \n",
    "\n",
    "tensor = torch.randn(2, 3, 256, 256)\n",
    "\n",
    "# padding last (width) dim by 1 on each side \n",
    "# (1, 1) = (1 + 1)\n",
    "# 256 + (1, 1) => 258\n",
    "pad1 = (1, 1)\n",
    "\n",
    "output = Padding(input=tensor,\n",
    "                pad=pad1,\n",
    "                mode='constant',\n",
    "                value=0)\n",
    "\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd72e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 258, 258])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch.nn.functional import pad as Padding \n",
    "\n",
    "tensor = torch.randn(2, 3, 256, 256)\n",
    "\n",
    "# padding last (width, height) dim by 1 on each side \n",
    "pad1 = (1, 1, 1, 1)\n",
    "\n",
    "output = Padding(input=tensor,\n",
    "                pad=pad1,\n",
    "                mode='constant',\n",
    "                )\n",
    "\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ded4528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 9, 258, 258])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch.nn.functional import pad as Padding \n",
    "\n",
    "tensor = torch.randn(2, 3, 8, 256, 256)\n",
    "\n",
    "# padding last (width, height, frame) dim by 1 on each side \n",
    "pad1 = (1, 1, 1, 1, 1, 0)\n",
    "\n",
    "output = Padding(input=tensor,\n",
    "                pad=pad1,\n",
    "                mode='constant',\n",
    "                )\n",
    "\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6cbe01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 10, 258, 258])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch.nn.functional import pad as Padding \n",
    "\n",
    "tensor = torch.randn(2, 3, 8, 256, 256)\n",
    "\n",
    "# padding last (width, height, frame, channels) dim by 1 on each side \n",
    "pad1 = (1, 1, 1, 1, 1, 1, 1, 1)\n",
    "\n",
    "output = Padding(input=tensor,\n",
    "                pad=pad1,\n",
    "                mode='constant',\n",
    "                )\n",
    "\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c6af7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 10, 258, 258])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch.nn.functional import pad as Padding \n",
    "\n",
    "tensor = torch.randn(2, 3, 8, 256, 256)\n",
    "\n",
    "# padding last (width, height, frame, channels, batch_size) dim by 1 on each side \n",
    "pad1 = (1, 1, 1, 1, 1, 1, 1, 1, 1, 0)\n",
    "\n",
    "output = Padding(input=tensor,\n",
    "                pad=pad1,\n",
    "                mode='constant',\n",
    "                )\n",
    "\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2e0985",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. QUESTION\n",
    "```\n",
    "    self.time_padding = self.dilation * (self.time_kernel_size - 1)  # 1*(3-1)=>2\n",
    "    self.height_padding = self.height_kernel_size // 2      # ≈1\n",
    "    self.width_padding = self.width_kernel_size // 2        # ≈1\n",
    "```\n",
    "\n",
    "why i can do this calculation to calculate the time_padding, height_padding and width_padding ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a6fb19",
   "metadata": {},
   "source": [
    "## ANSWER ->\n",
    "\n",
    "\n",
    "The provided code snippets are likely from a deep learning model, specifically a convolutional neural network (CNN) that processes spatio-temporal data (data with height, width, and time dimensions, like video). The lines you've shared are calculating the padding needed for the convolutional layers.\n",
    "\n",
    "Let's break down why these calculations work and what padding is for:\n",
    "\n",
    "### What is Padding?\n",
    "\n",
    "In a convolutional layer, a kernel (or filter) slides over the input data. At each step, it performs a dot product, and the result becomes a single pixel in the output feature map. Without padding, the output feature map is always smaller than the input.\n",
    "\n",
    "For example, a `3x3` kernel on a `5x5` input produces a `3x3` output.\n",
    "\n",
    "$$(5 - 3) + 1 = 3$$\n",
    "\n",
    "This reduction in size can be problematic:\n",
    "1.  **Loss of information at the borders:** The pixels at the edges and corners of the input are \"seen\" fewer times by the kernel, meaning their information is less represented in the output.\n",
    "2.  **Shrinking feature maps:** Stacking multiple convolutional layers without padding would quickly shrink the feature maps to a point where they disappear.\n",
    "\n",
    "Padding involves adding extra rows and columns of values (usually zeros) around the input data before the convolution. This allows you to control the size of the output feature map.\n",
    "\n",
    "There are two common types of padding:\n",
    "\n",
    "1.  **\"Valid\" padding:** No padding is used. The output size is smaller than the input.\n",
    "2.  **\"Same\" padding:** Padding is added so that the output size is the same as the input size. This is what your code is likely aiming for.\n",
    "\n",
    "### Breaking Down the Calculations\n",
    "\n",
    "The goal of \"same\" padding is to ensure that the output dimension matches the input dimension. The general formula for the output dimension (for a single dimension) is:\n",
    "\n",
    "$$Output\\ Dimension = \\lfloor \\frac{Input\\ Dimension + 2 \\times Padding - Dilation \\times (Kernel\\ Size - 1) - 1}{Stride} \\rfloor + 1$$\n",
    "\n",
    "For \"same\" padding with a stride of 1, we want the `Output Dimension` to equal the `Input Dimension`. This simplifies the formula.\n",
    "\n",
    "Let's analyze each of your lines:\n",
    "\n",
    "#### `self.time_padding = self.dilation * (self.time_kernel_size - 1)`\n",
    "\n",
    "* **`self.time_kernel_size`**: The size of the kernel in the time dimension.\n",
    "* **`self.dilation`**: The dilation rate. Dilation is a technique that expands the kernel by inserting spaces between its elements. A dilation of 1 means a standard convolution. A dilation of 2 means the kernel skips a pixel, effectively looking at a larger field of view.\n",
    "\n",
    "The formula for \"same\" padding with dilation is:\n",
    "\n",
    "$$Padding = \\frac{Dilation \\times (Kernel\\ Size - 1)}{2}$$\n",
    "\n",
    "The code snippet seems to be missing a division by 2, or it's a specific implementation where the padding is applied unevenly (e.g., `(Kernel Size - 1)` on one side and the rest on the other, or where the padding is an integer and the framework handles it).\n",
    "\n",
    "Let's look at the example you provided: `1*(3-1) => 2`.\n",
    "If the kernel size is 3 and the dilation is 1, the padding is 2. This suggests a total padding of 2 is needed. A common setup is to apply 1 padding on the left and 1 on the right to keep the output size the same. The formula `(Kernel Size - 1)` gives you the total padding needed for a stride of 1.\n",
    "\n",
    "For example, a `1D` input of size `7` and a kernel of size `3`:\n",
    "* `7 - 3 + 1 = 5`. The output is size `5`.\n",
    "* To get an output of size `7`, we need to pad. The formula `(3 - 1) = 2` suggests a total padding of 2. If we add 1 padding to each side, the input becomes size `9`.\n",
    "* `9 - 3 + 1 = 7`. The output is size `7`, which is the same as the input.\n",
    "\n",
    "So, `self.time_padding` is the total padding needed in the time dimension to maintain the input size, considering the effect of dilation.\n",
    "\n",
    "#### `self.height_padding = self.height_kernel_size // 2`\n",
    "#### `self.width_padding = self.width_kernel_size // 2`\n",
    "\n",
    "These are for the spatial dimensions (height and width). In standard 2D convolutions, dilation is usually 1.\n",
    "\n",
    "The general \"same\" padding formula for a 2D convolution with a stride of 1 is:\n",
    "\n",
    "$$Padding\\ (per\\ side) = \\lfloor \\frac{Kernel\\ Size - 1}{2} \\rfloor$$\n",
    "\n",
    "Since the `height_kernel_size` and `width_kernel_size` are likely odd numbers (like 3 or 5), the integer division `// 2` is equivalent to the floor division.\n",
    "\n",
    "* If `self.height_kernel_size` is 3, then `3 // 2 = 1`. You need 1 padding on each side (top and bottom) for a total of 2. The formula `(Kernel Size - 1)` gives you 2. `2 // 2 = 1`.\n",
    "* If `self.height_kernel_size` is 5, then `5 // 2 = 2`. You need 2 padding on each side for a total of 4. The formula `(Kernel Size - 1)` gives you 4. `4 // 2 = 2`.\n",
    "\n",
    "This calculation is a common and correct way to determine the padding required for \"same\" convolution when the kernel size is an odd number.\n",
    "\n",
    "### Summary\n",
    "\n",
    "In short, you can do these calculations because they are the standard mathematical formulas for determining the amount of padding needed to ensure that a convolutional layer's output has the same dimensions as its input.\n",
    "\n",
    "* `self.dilation * (self.time_kernel_size - 1)` calculates the total padding needed in the time dimension to counteract the size reduction caused by the convolution and dilation.\n",
    "* `self.height_kernel_size // 2` and `self.width_kernel_size // 2` calculate the padding needed *per side* for the height and width dimensions, assuming the kernel sizes are odd."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e54954",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "# 9. is this calculation to related to paper ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d616f1f",
   "metadata": {},
   "source": [
    "### ANS -> \n",
    "Yes, the calculations are directly related to the theory and standard practices of convolutional neural networks (CNNs), which are documented in numerous research papers and deep learning textbooks.\n",
    "\n",
    "The calculations you provided are not tied to one specific, single paper, but rather are a fundamental part of the \"convolutional arithmetic\" that underpins modern CNN architectures. This arithmetic has been refined and popularized over the years through works that have introduced new network designs and techniques.\n",
    "\n",
    "Here's how those calculations relate to the broader field:\n",
    "\n",
    "### 1. \"Same\" Padding\n",
    "\n",
    "The calculations `self.height_padding = self.height_kernel_size // 2` and `self.width_padding = self.width_kernel_size // 2` are the standard way to implement \"same\" padding for convolutions with a stride of 1. The goal is to ensure the output feature map has the same dimensions (height and width) as the input. This practice is widely used in many popular CNN architectures, such as VGG and ResNet, to prevent the spatial dimensions from shrinking too quickly.\n",
    "\n",
    "* **Why it's a \"thing\"**: Before the widespread use of \"same\" padding, early CNNs (like LeNet) would shrink the feature maps with each layer, which limited the number of layers you could stack. The introduction of padding as a standard practice allowed for the development of much deeper networks, which led to significant performance improvements.\n",
    "\n",
    "### 2. Dilated Convolution\n",
    "\n",
    "The calculation `self.time_padding = self.dilation * (self.time_kernel_size - 1)` is the padding formula for a dilated convolution. This is a more advanced technique that was formalized in papers like \"[Multi-scale Context Aggregation by Dilated Convolutions](https://arxiv.org/abs/1511.07122)\" (by Fisher Yu and Vladlen Koltun, 2016).\n",
    "\n",
    "* **Why it's a \"thing\"**: Dilated convolutions, also known as \"atrous\" convolutions, allow a convolutional layer to have a wider \"receptive field\" (the area of the input it \"sees\") without increasing the number of parameters or the computational cost. The padding formula is crucial here to maintain the output size while accommodating the gaps introduced by dilation. The formula `dilation * (kernel_size - 1)` calculates the effective kernel size, and the padding is applied to compensate for this. This technique is especially useful in tasks like semantic segmentation where you need a large receptive field to understand context without losing spatial resolution.\n",
    "\n",
    "In summary, the calculations you are using are standard practices that stem from the foundational research and engineering advancements in deep learning. They are part of the core \"convolutional arithmetic\" that is essential for designing and implementing modern neural networks. You would find these formulas in deep learning frameworks' documentation (like PyTorch or TensorFlow) and in numerous academic papers that build on these concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f0ff63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 8, 256, 256])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "conv_3d = nn.Conv3d(in_channels=128,\n",
    "                    out_channels=256,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    )\n",
    "\n",
    "x = torch.randn(2, 128, 8, 256, 256)\n",
    "out = conv_3d(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0869f386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 6, 254, 254])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "conv_3d = nn.Conv3d(in_channels=128,\n",
    "                    out_channels=256,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    )\n",
    "\n",
    "x = torch.randn(2, 128, 8, 256, 256)\n",
    "out = conv_3d(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7c46728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "x = torch.randn(2, 3, 8, 256, 256)\n",
    "\n",
    "b, c, t, h, w = x.shape\n",
    "\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bece2d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
